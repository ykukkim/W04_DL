{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Whole_seq_Testing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMoxxu1tYTyyxlMjtxP8F8+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_DuV2Tc7le2Z","colab_type":"text"},"source":["# Directory"]},{"cell_type":"code","metadata":{"id":"IVRtMCpPliFh","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2HwYTxailkXh","colab_type":"code","outputId":"670425b1-6a19-45bf-c664-d51a0a7e7151","executionInfo":{"status":"ok","timestamp":1587555239159,"user_tz":-120,"elapsed":2689,"user":{"displayName":"Yong Kuk Kim","photoUrl":"","userId":"05925217294532758280"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd /content/drive/My\\ Drive/Colab\\ Notebooks"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MYFNTrHTl1R-","colab_type":"text"},"source":["# Libraries"]},{"cell_type":"code","metadata":{"id":"adROerAMl3b1","colab_type":"code","colab":{}},"source":["# Importing Libraries and Packages\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader, Dataset\n","\n","# calculate train time, writing train data to files etc.\n","import os\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","from scipy import signal\n","import pdb"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GR2e_cOvl4Vf","colab_type":"text"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"v1H5JxBbl6Me","colab_type":"code","colab":{}},"source":["# Extracting data from csv files\n","class CoolDataset(Dataset):\n","\n","    # Initialise your data, download etc\n","    def __init__(self, dir_path: str, input_size: str, convolution=True):\n","        super().__init__()\n","\n","        self.files = tuple(Path(dir_path).glob(\"**/*.csv\"))\n","        self.input_size = input_size\n","        self.window = signal.gaussian(8, std=3)\n","        self.convolution = convolution\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","\n","    def __getitem__(self, idx):\n","        if idx < len(self.files):\n","            data, filename = self.read_file(self.files[idx])\n","\n","            indx = data.iloc[:, 0].to_numpy()\n","            input = data.iloc[0:, 0:self.input_size].to_numpy()\n","            output = data.iloc[:]['FS'].to_numpy()\n","\n","            if self.convolution:\n","                output = signal.convolve(output, self.window, mode='same')\n","\n","            input_data = torch.as_tensor(torch.from_numpy(input).float())\n","            output_data = torch.as_tensor(torch.from_numpy(output).float())\n","            assert input_data.shape[0] == output_data.shape[0]\n","        return indx, input_data, output_data, filename\n","\n","        \n","    def read_file(self, f):\n","\n","        df = pd.read_csv(open(f, \"r\"))\n","        fname = os.path.basename(f)\n","        if fname[0:2] == 'RT':\n","          df = df.drop(['ID','LHEE_X','LHEE_Y',\t'LHEE_Z',\t'LTOE_X',\t'LTOE_Y',\t'LTOE_Z',\t'LHLX_X',\t'LHLX_Y',\t'LHLX_Z',\t'LPMT5_X',\t'LPMT5_Y',\t'LPMT5_Z',\n","                      'LPMT1_X',\t'LPMT1_Y',\t'LPMT1_Z',\t'LDMT1_X',\t'LDMT1_Y',\t'LDMT1_Z',\t'LDMT5_X',\t'LDMT5_Y',\t'LDMT5_Z',\t'LVHEE_X',\t'LVHEE_Y',\t\n","                      'LVHEE_Z',\t'LVTOE_X',\t'LVTOE_Y',\t'LVTOE_Z',\t'LVHLX_X','LVHLX_Y',\t'LVHLX_Z',\t'LVPMT5_X',\t'LVPMT5_Y',\t'LVPMT5_Z',\t'LVPMT1_X',\t\n","                      'LVPMT1_Y',\t'LVPMT1_Z',\t'LVDMT1_X',\t'LVDMT1_Y',\t'LVDMT1_Z',\t'LVDMT5_X',\t'LVDMT5_Y',\t'LVDMT5_Z','SACR_LHEE',\t'SACR_LTOE',\t\n","                      'SACR_LHLX',\t'SACR_LPMT5',\t'SACR_LPMT1',\t'SACR_LDMT1',\t'SACR_LDMT5','RVDMT5_X','RVPMT5_X','RVDMT1_X','RVPMT1_X','RVHLX_X','RVTOE_X','RVHEE_X'],axis = 1)\n","        elif fname[0:2] == 'LT':\n","          df = df.drop(['ID','RHEE_X','RHEE_Y',\t'RHEE_Z',\t'RTOE_X',\t'RTOE_Y',\t'RTOE_Z','RHLX_X',\t'RHLX_Y',\t'RHLX_Z',\t'RPMT5_X',\t'RPMT5_Y',\t'RPMT5_Z',\n","                      'RPMT1_X',\t'RPMT1_Y',\t'RPMT1_Z',\t'RDMT1_X',\t'RDMT1_Y','RDMT1_Z',\t'RDMT5_X',\t'RDMT5_Y',\t'RDMT5_Z',\t'RVHEE_X',\t'RVHEE_Y',\t\n","                      'RVHEE_Z',\t'RVTOE_X',\t'RVTOE_Y',\t'RVTOE_Z',\t'RVHLX_X','RVHLX_Y',\t'RVHLX_Z',\t'RVPMT5_X',\t'RVPMT5_Y',\t'RVPMT5_Z',\t'RVPMT1_X',\t\n","                      'RVPMT1_Y',\t'RVPMT1_Z',\t'RVDMT1_X',\t'RVDMT1_Y',\t'RVDMT1_Z',\t'RVDMT5_X',\t'RVDMT5_Y',\t'RVDMT5_Z','SACR_RHEE',\t'SACR_RTOE',\t\n","                      'SACR_RHLX',\t'SACR_RPMT5',\t'SACR_RPMT1',\t'SACR_RDMT1',\t'SACR_RDMT5','LVDMT5_X','LVPMT5_X','LVDMT1_X','LVPMT1_X','LVHLX_X','LVTOE_X','LVHEE_X'],axis = 1)\n","        return df,fname"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ud8cZtfql_xp","colab_type":"text"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"Q_z1DQgUmBFT","colab_type":"code","colab":{}},"source":["class Network(nn.Module):\n","    # TO DO\n","    def __init__(self, config):\n","        super(Network, self).__init__()\n","\n","        # Model construct Configuration\n","        self.input_size = config.input_size\n","        self.hidden_size = config.hidden_size\n","        self.output_size = config.output_size\n","        self.batch_size = config.batch_size\n","        self.num_layers = config.num_layers\n","        self.drop_out = config.drop_out\n","        self.device = config.device\n","\n","        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, dropout=self.drop_out, batch_first=True,bidirectional=True)\n","        self.linear = nn.Linear(self.hidden_size * 2, self.output_size)\n","\n","    def forward(self, x):\n","        hidden, cell = self.init_hidden()\n","        out, (hn, cn) = self.lstm(x, (hidden, cell))\n","        logits = self.linear(out)\n","\n","        return logits[:, :, -1]\n","\n","    def init_hidden(self):\n","        weight = next((self.parameters())).data\n","\n","        hidden, cell = (weight.new(self.num_layers * 2, self.batch_size, self.hidden_size).zero_().to(self.device),\n","                        weight.new(self.num_layers * 2, self.batch_size, self.hidden_size).zero_().to(self.device))\n","        return hidden, cell\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5gVRXlhwq7zV","colab_type":"text"},"source":["# Peak Detection"]},{"cell_type":"code","metadata":{"id":"5kfCzg4alWxW","colab_type":"code","colab":{}},"source":["class peak_detection:\n","\n","    def peak_comp(self, annotated, predicted):\n","        dist = []\n","\n","        if len(predicted) == 0 or len(annotated) == 0:\n","            return -1\n","\n","        # if len(predicted) != len(annotated):\n","        #     return -1\n","\n","        for a in annotated:\n","            dist = dist + [min(np.abs(predicted - a))]\n","\n","        if not len(dist):\n","            return -1\n","\n","        return min(dist)\n","\n","    def width_comp(self, annotated, predicted):\n","        width = []\n","        pred_diff = np.abs(predicted[3]-predicted[2])\n","        true_diff = np.abs(annotated[3]-annotated[2])\n","\n","        if len(true_diff) == 0 or len(pred_diff) == 0:\n","            return -1\n","\n","        # if len(predicted) != len(annotated):\n","        #     return -1\n","\n","        for a in true_diff:\n","            width = width + [min(np.abs(a - pred_diff))]\n","\n","        if not len(width):\n","            return -1\n","\n","        return min(width)\n","\n","    def eval_prediction(self, y_pred, y_true, filename, plot=True, shift=0):\n","        sdist = []\n","        swidth = []\n","\n","        peakind, _ = signal.find_peaks(y_pred,0.9)\n","        peakind_true, _ = signal.find_peaks(y_true,0.9)\n","        results = signal.peak_widths(y_pred, peakind, rel_height=0.5)\n","        results_true = signal.peak_widths(y_true, peakind_true, rel_height=0.5)\n","\n","        for k in peakind:\n","            if plot:\n","                plt.axvline(x=k)\n","        sdist.append(self.peak_comp(self, peakind_true, [k + shift for k in peakind]))\n","        swidth.append(self.width_comp(self, results_true, [k for k in results]))\n","\n","        if plot:\n","            plt.plot(y_pred)\n","            plt.plot(y_true)\n","            plt.title(filename)\n","            axes = plt.gca()\n","            axes.set_xlim([0, y_true.shape[0]])\n","            my_file = f\"{filename[0][0:-9]}\"\n","            plt.savefig(os.path.join(self.png_dir, my_file))\n","            plt.close()\n","\n","        return sdist, swidth\n","\n","    def plot_stats(self, sdist,filename):\n","        plt.hist(sdist, 100, [0, 100])\n","        filtered = [k for k in sdist if k >= 0]\n","\n","        def off_by(threshold, filtered):\n","            ob = [k for k in filtered if k <= threshold]\n","            nel = float(len(filtered))\n","            print(\"<= %d: %f\" % (threshold, len(ob) / float(nel)))\n","\n","        print(\"Error distribution:\")\n","        off_by(1, filtered)\n","        off_by(3, filtered)\n","        off_by(5, filtered)\n","        off_by(10, filtered)\n","        off_by(60, filtered)\n","        print(\"Mean distance: %f\" % (np.mean(filtered)))\n","        plt.savefig(os.path.join(self.png_dir,f\"{filename[0][0:-9]}_distance_error.png\"))\n","        plt.close()\n","\n","    def plot_width(self, swidth,filename):\n","        plt.hist(swidth, 100, [0, 100])\n","        filtered = [k for k in swidth if k >= 0]\n","\n","        def off_by(threshold, filtered):\n","            ob = [k for k in filtered if k <= threshold]\n","            nel = float(len(filtered))\n","            print(\"<= %d: %f\" % (threshold, len(ob) / float(nel)))\n","\n","        print(\"Width Error distribution:\")\n","        off_by(1, filtered)\n","        off_by(3, filtered)\n","        off_by(5, filtered)\n","        off_by(10, filtered)\n","        off_by(60, filtered)\n","        print(\"Mean Width: %f\" % (np.mean(filtered)))\n","        plt.savefig(os.path.join(self.png_dir,f\"{filename[0][0:-9]}_Width_error.png\"))\n","        plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sF9N6Wvbq6l2","colab_type":"text"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"af6fOkm-rBfC","colab_type":"code","colab":{}},"source":["class Trainer:\n","\n","    def __init__(self, model, config):\n","\n","        # System configuration\n","        self.device = config.device\n","\n","        # Model Construction\n","        self.model = Network(config).float()\n","        self.model.load_state_dict(model)\n","        self.model.to(self.device)\n","        print(self.model)\n","\n","        # Peak detection and Evaluation\n","        self.eval_prediction = peak_detection.eval_prediction\n","        self.peak_comp = peak_detection.peak_comp\n","        self.width_comp = peak_detection.width_comp\n","        self.plot_stats = peak_detection.plot_stats\n","        self.plot_width = peak_detection.plot_width\n","        self.output_dir = config.output_dir\n","        self.png_dir = config.png_dir\n","        self.globaliter = 0\n","\n","        # DataLoader\n","        self.test_loader = DataLoader(CoolDataset(r\"data/iteration3/test/\", config.input_size,convolution=True),batch_size=config.batch_size, drop_last=True, shuffle=False)\n","\n","    def test(self):\n","\n","        self.model.eval()\n","        with torch.no_grad():\n","\n","            for indx, data, target,filename in self.test_loader:\n","                error_dist = []\n","                error_width = []\n","\n","                data, target=  data.to(self.device), target.to(self.device)\n","\n","                predictions = self.model(data.float())\n","                pred = torch.sigmoid(predictions)\n","                print(filename)\n","                for i in range(0, pred.shape[0]):\n","                    dist, width = (self.eval_prediction(self, pred[i], target[i], filename))\n","                    error_dist.extend(dist)\n","                    error_width.extend(width)\n","                    \n","\n","                self.plot_stats(self, error_dist,filename)\n","                self.plot_width(self, error_width,filename)\n","\n","                dist_file = np.column_stack((error_dist, filename))\n","                width_file = np.column_stack((error_width, filename))\n","                df_dist = pd.DataFrame(data=dist_file, )\n","                df_width = pd.DataFrame(data=width_file, )\n","                df_dist.columns = [\"Error in distance\", \"FileName\"]\n","                df_width.columns = [\"Error in width\", \"FileName\"]\n","                my_file = f\"{filename[0][0:-9]}\"\n","                df_dist.to_csv(os.path.join(self.output_dir, f\"{my_file}_dist.csv\"))\n","                df_width.to_csv(os.path.join(self.output_dir, f\"{my_file}_width.csv\"))\n","\n","                for i in range(0, predictions.shape[0]):\n","                    indx_pred_target = np.transpose(np.row_stack((indx[i], pred[i], target[i])))\n","                    df = pd.DataFrame(data=indx_pred_target, )\n","                    df.columns = [\"indx\", \"pred\", \"true_output\"]\n","                    df.to_csv(os.path.join(self.output_dir, f\"{my_file}.csv\"))\n","\n","                self.globaliter += 1\n","\n","            "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GooedGGhrEXy","colab_type":"text"},"source":["# Main"]},{"cell_type":"code","metadata":{"id":"tpDw1uvl2HKk","colab_type":"code","colab":{}},"source":["class Config:\n","\n","    def __init__(self, **kwargs):\n","        for key, value in kwargs.items():\n","            setattr(self, key, value)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6qeT72PRrGLy","colab_type":"code","outputId":"2d65c026-4bc9-4f54-ed38-bd7cbdfe24fd","executionInfo":{"status":"error","timestamp":1587555254866,"user_tz":-120,"elapsed":2316,"user":{"displayName":"Yong Kuk Kim","photoUrl":"","userId":"05925217294532758280"}},"colab":{"base_uri":"https://localhost:8080/","height":429}},"source":["def main(loaded_model, model_config):\n","    trainer = Trainer(loaded_model,  model_config)\n","    trainer.test()\n","\n","if __name__ == '__main__':\n","\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","        print(\"Running on the GPU\")\n","    else:\n","        device = torch.device(\"cpu\")\n","        print(\"Running on the CPU\")\n","\n","    \"Model\"\n","    model_files = tuple(Path(r'data/TrainedModel/FS').glob(\"**/*pt\"))\n","    model_name = os.path.basename((model_files[0]))\n","    print(model_name)\n","    loaded_model = torch.load(r'data/TrainedModel/FS' + model_name, map_location=torch.device('cpu'))\n","    \n","    model_config = Config(\n","          device=device,\n","          input_size=42,\n","          batch_size=1,\n","          hidden_size=512,\n","          num_layers=5,\n","          lr=0.0001,\n","          drop_out=0.2,\n","          output_size=1,\n","          output_dir=r'data/output/csv/',\n","          png_dir=r'data/output/pngs/',\n","      )\n","    main(loaded_model, model_config)\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Running on the CPU\n","FS-1-SP-13-WF-42-IS-64-BS-128-SL-512-HS-5-NL-0.001-LR-200-epochs-FScheckpoint.pt\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-6e2432c90c37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'data/TrainedModel/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     model_config = Config(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/TrainedModel/FS-1-SP-13-WF-42-IS-64-BS-128-SL-512-HS-5-NL-0.001-LR-200-epochs-FScheckpoint.pt'"]}]},{"cell_type":"code","metadata":{"id":"fLXZXo6F-y1d","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}