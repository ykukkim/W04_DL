{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IC_3_TestingModelFPR.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FVe4m9BcQgv7"},"source":["# Directory"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bHfztK9GQk94","executionInfo":{"status":"ok","timestamp":1638139370160,"user_tz":-60,"elapsed":44876,"user":{"displayName":"Yong Kuk Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05925217294532758280"}},"outputId":"36f8a81e-cbbe-4f31-ed57-7fe2088c0e6b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uyaLNUQ3Qnnh","executionInfo":{"status":"ok","timestamp":1638139370161,"user_tz":-60,"elapsed":5,"user":{"displayName":"Yong Kuk Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05925217294532758280"}},"outputId":"6f3d3df9-bc9b-403d-eeb2-7b2bd832567f"},"source":["%cd /content/drive/MyDrive/Deep_learning/Deep\\ Learning"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Deep_learning/Deep Learning\n"]}]},{"cell_type":"markdown","metadata":{"id":"l-6pQj7zQoc9"},"source":["# Libararies"]},{"cell_type":"code","metadata":{"id":"5g_eD6eiQsUS"},"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, IterableDataset\n","\n","# calculate train time, writing train data to files etc.\n","import os\n","import sys\n","import logging\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","\n","from random import randint\n","from collections import defaultdict\n","from pathlib import Path\n","from scipy import signal\n","from scipy.signal import find_peaks\n","from scipy.interpolate import interp1d,UnivariateSpline\n","from sklearn.preprocessing import MinMaxScaler\n","from numpy import NaN, Inf, arange, isscalar, asarray, array\n","\n","import pdb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O4v4l7XuQ019"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"YrNpeb2GQ2KS"},"source":["class CoolDataset(IterableDataset):\n","\n","    def __init__(self, dir_path: str, seq_length: str, input_size: str, samples_per_events: str, convolution=True):\n","\n","        # Opens .csv file and read headers\n","        # Needs to decide what inputs to have and work on HS or FO\n","\n","        super().__init__()\n","\n","        self.files = tuple(Path(dir_path).glob(\"**/*.csv\"))\n","        self.seq_length = seq_length\n","        self.input_size = input_size\n","        self.SAMPLES_PER_EVENT = samples_per_events\n","        self.window = signal.gaussian(8, std=3)\n","        self.convolution = convolution\n","        self.scaler = MinMaxScaler(feature_range=(0,1))\n","\n","        assert seq_length % 2 == 0, \"Please pass an even seq length\"\n","\n","    def __iter__(self):\n","\n","        # Initialise Counter for events and files\n","        self.file_nr = 0\n","        self.event_in_file = 0\n","        self._sample_nr = 0\n","\n","        return self\n","\n","    def __next__(self):\n","        # Reads the current file and looks for event\n","        df, fname = self.read_file(self.files[self.file_nr])  # could be cached so you dont read it anew every iteration\n","        events = df[df['FS'] == 1]  # true where an event occurs, false everywhere else, use as mask\n","\n","        if events.shape[0] > 0:\n","            if self._sample_nr < self.SAMPLES_PER_EVENT:\n","                # just give back the current event again, with different sampling, until we have generated\n","                # SAMPLES_PER_EVENT such samples\n","\n","                event_frame = events.iloc[self.event_in_file].name\n","                indx_data, input_data, output_data = self.sample_seq_around_event_frame(df, event_frame)\n","                self._sample_nr += 1\n","            else:\n","                self.event_in_file += 1  # work on the next event in this file\n","                self._sample_nr = 0  # reset for the next event\n","\n","                # check whether we are done with this file\n","                # otherwise we return the next event on the beginning of the next iteration\n","                if self.event_in_file >= len(events):\n","                    self.file_nr += 1\n","                    # If there still are files to run, it resets the variables\n","                    if self.file_nr < len(self.files):\n","                        logging.info(\"File is complete. Going to new file...\")\n","                        self.event_in_file = 0\n","                        self._sample_nr = 0\n","                        return next(self)\n","                    else:\n","                        # processed the last file, we are done\n","                        logging.info(\"File is complete. All files done. Stopping...\")\n","                        raise StopIteration\n","                elif self.event_in_file < len(events):\n","                    event_frame = events.iloc[self.event_in_file].name\n","                    indx_data, input_data, output_data = self.sample_seq_around_event_frame(df, event_frame)\n","                    self._sample_nr += 1\n","        else:\n","            logging.info(\"No events detected\")\n","            self.file_nr += 1\n","            self.event_in_file = 0\n","            self._sample_nr = 0\n","            return next(self)\n","\n","        return indx_data, input_data, output_data, fname\n","\n","    def sample_seq_around_event_frame(self, df, event_idx):\n","\n","        if event_idx >= 4:\n","            start_idx = event_idx - randint(4, self.seq_length / 7.5)\n","            if start_idx > 0:\n","                end_idx = start_idx + self.seq_length\n","                if end_idx <= len(df):\n","                    indx = df.iloc[start_idx:end_idx]['ID']\n","                    input = df.iloc[start_idx:end_idx, 1:self.input_size + 1]\n","                    output = df.iloc[start_idx:end_idx]['FS']\n","                elif end_idx > len(df):\n","                    end_idx = len(df)\n","                    start_idx = end_idx - self.seq_length\n","                    indx = df.iloc[start_idx:end_idx]['ID']\n","                    input = df.iloc[start_idx:end_idx, 1:self.input_size + 1]\n","                    output = df.iloc[start_idx:end_idx]['FS']\n","            elif start_idx <= 4:\n","                start_idx = event_idx\n","                end_idx = start_idx + self.seq_length\n","                indx = df.iloc[start_idx:end_idx]['ID']\n","                input = df.iloc[start_idx:end_idx, 1:self.input_size + 1]\n","                output = df.iloc[start_idx:end_idx]['FS']\n","                if end_idx <= len(df):\n","                    indx = df.iloc[start_idx:end_idx]['ID']\n","                    input = df.iloc[start_idx:end_idx, 1:self.input_size + 1]\n","                    output = df.iloc[start_idx:end_idx]['FS']\n","                elif end_idx > len(df):\n","                    end_idx = len(df)\n","                    start_idx = end_idx - self.seq_length\n","                    indx = df.iloc[start_idx:end_idx]['ID']\n","                    input = df.iloc[start_idx:end_idx, 1:self.input_size + 1]\n","                    output = df.iloc[start_idx:end_idx]['FS']\n","\n","        if self.convolution:\n","            output = signal.convolve(output, self.window, mode='same')\n","        input = self.scaler.fit_transform(input)\n","        indx = indx.to_numpy()\n","        \n","        assert input.shape[0] == output.shape[0] == self.seq_length\n","        return torch.tensor(indx),torch.tensor(input), torch.tensor(output)\n","\n","    def read_file(self, f):\n","        df = pd.read_csv(open(f, \"r\"))\n","        fname = os.path.basename(f)\n","        if fname[0:2] == 'RT':\n","            df = df.drop([#'ID',\n","                          'class',\n","                          #'RTOE_X', 'RTOE_Y', 'RTOE_Z', 'V_RTOE_X', 'V_RTOE_Y', 'V_RTOE_Z',\n","                          'RHLX_X', 'RHLX_Y', 'RHLX_Z', 'V_RHLX_X', 'V_RHLX_Y', 'V_RHLX_Z',\n","                          #'RHEE_X', 'RHEE_Y', 'RHEE_Z', 'V_RHEE_X', 'V_RHEE_Y', 'V_RHEE_Z',\n","                          'RPMT5_X', 'RPMT5_Y', 'RPMT5_Z', 'V_RPMT5_X', 'V_RPMT5_Y', 'V_RPMT5_Z'\n","                          ], axis=1)\n","        elif fname[0:2] == 'LT':\n","            df = df.drop([#'ID',\n","                          'class',\n","                          #'LTOE_X', 'LTOE_Y', 'LTOE_Z', 'V_LTOE_X', 'V_LTOE_Y', 'V_LTOE_Z',\n","                          'LHLX_X', 'LHLX_Y', 'LHLX_Z', 'V_LHLX_X', 'V_LHLX_Y', 'V_LHLX_Z',\n","                          #'LHEE_X', 'LHEE_Y', 'LHEE_Z', 'V_LHEE_X', 'V_LHEE_Y', 'V_LHEE_Z',\n","                          'LPMT5_X', 'LPMT5_Y', 'LPMT5_Z', 'V_LPMT5_X', 'V_LPMT5_Y', 'V_LPMT5_Z',\n","                          ], axis=1)\n","        return df, fname"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L3zHwEfZQ4Px"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"LjBwE7oPQ5Gv"},"source":["class Network(nn.Module):\n","    # TO DO\n","    def __init__(self, config):\n","        super(Network, self).__init__()\n","\n","        # Model construct Configuration\n","        self.batch_size  = config.batch_size\n","        self.num_layers  = config.num_layers\n","        self.hidden_size = config.hidden_size\n","        self.device      = config.device\n","\n","        self.lstm = nn.LSTM(config.input_size, self.hidden_size, self.num_layers, dropout=config.drop_out, batch_first=True,\n","                            bidirectional=True)\n","        self.linear = nn.Linear(self.hidden_size * 2, config.output_size, bias=True)\n","        torch.nn.init.xavier_uniform_(self.linear.weight)\n","\n","    def forward(self, x):\n","        hidden, cell = self.init_hidden()\n","        out, _ = self.lstm(x, (hidden, cell))\n","        logits = self.linear(out)\n","\n","        return logits[:, :, -1]\n","\n","    def init_hidden(self):\n","        weight = next((self.parameters())).data\n","        hidden, cell = (weight.new(self.num_layers * 2, self.batch_size, self.hidden_size).zero_().to(self.device),\n","                        weight.new(self.num_layers * 2, self.batch_size, self.hidden_size).zero_().to(self.device))\n","        return hidden, cell"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oe0Yx2HjRBw2"},"source":["# Peak Detection"]},{"cell_type":"code","metadata":{"id":"rSt3MhKdRC5O"},"source":["class peak_detection:\n","\n","    def peak_comp(self, annotated, predicted, filename):\n","        true_detect = np.abs(annotated -predicted) < 2.4\n","        temp_idx = np.where(true_detect)\n","        pred_peaks_close_to_true = true_detect.sum()\n","        pred_peaks_not_close_to_true = len(true_detect) - true_detect.sum()\n","        dist = np.abs(annotated-predicted)\n","        # if (len(temp_idx) != 0):\n","        #    dist = [np.abs(annotated - np.array(predicted)[i.astype(int)]) for i in temp_idx]\n","        # elif (len(temp_idx) == 0):\n","        #     print(f\"{filename}_Events Outside of 16ms\")\n","        #     dist = np.abs(annotated - predicted)\n","        #     return dist, 0, len(predicted)\n","\n","        return dist,pred_peaks_close_to_true,pred_peaks_not_close_to_true\n","\n","    def eval_prediction(self, y_pred, y_true, filename, i, plot=True, shift=0):\n","\n","        # sdist = []\n","        # Interpolating the predicted signal\n","        y_pred_interp1d = interp1d(self.x, y_pred)\n","        y_pred_final = y_pred_interp1d(self.xnew)\n","\n","        try:\n","\n","            # peak_detection of true\n","            peakind_true,_ = find_peaks(y_true.numpy(), height=0.5)\n","            if peakind_true.size == 0:\n","                y_temp = np.where(y_true.numpy()>0.9)\n","                peakind_true = np.array([y_temp[0][0]])\n","\n","            # peak_detection of predicted\n","            peakind,_ = find_peaks(y_pred_final, height=0.5)\n","            peakind_pred = [x for x in peakind]\n","            peakind_pred = self.xnew[peakind_pred]\n","\n","            if peakind_pred.size == 0:\n","                y_temp_pred = np.where(y_pred_final > 0.9)\n","                peakind_pred = np.array([y_temp_pred[0][0]])\n","\n","\n","            # if any(peakind_pred) and any(peakind_true):\n","            #     for k in peakind_pred:\n","            #         if plot:\n","            #             plt.axvline(x=k)\n","\n","            sdist, freq_tpr, freq_fpr = self.peak_comp(self, peakind_true, peakind_pred, filename[:-4])\n","\n","            # sdist.append(self.peak_comp(self, peakind_true, [k + shift for k in peakind_pred], filename[:-4]))\n","\n","            if plot:\n","                plt.plot(y_pred)\n","                plt.plot(y_true)\n","                plt.title(f\"Gold Standard vs Model\")\n","                axes = plt.gca()\n","                axes.set_xlim([0, y_true.shape[0]])\n","                my_file = f\"{filename[:-4]} - {i}.pdf\"\n","                plt.savefig(os.path.join(self.png_dir, my_file))\n","                plt.close()\n","\n","        except:\n","            print(f\"{filename} - No Events\")\n","            sdist = [np.abs(peakind_true-peakind_true)-1]\n","            freq_tpr = -1\n","            freq_fpr = -1\n","\n","        return sdist,freq_tpr,freq_fpr, filename\n","\n","    def plot_stats(self, sdist):\n","        plt.title(\"Distance Error Histogram\")\n","        plt.hist(sdist, 100, [0, 100])\n","        filtered = [k for k in sdist if k >= 0]\n","\n","        def off_by(threshold, filtered):\n","            ob = [k for k in filtered if k <= threshold]\n","            nel = float(len(filtered))\n","            print(\"<= %d: %f\" % (threshold, len(ob) / float(nel)))\n","\n","        print(\"Error distribution:\")\n","        off_by(1, filtered)\n","        off_by(3, filtered)\n","        off_by(5, filtered)\n","        off_by(10, filtered)\n","        off_by(60, filtered)\n","        print(\"Mean distance: %f\" % (np.mean(filtered)))\n","        mean_distance = [np.mean(filtered)]\n","        plt.legend(mean_distance)\n","        plt.savefig(os.path.join(self.png_dir, f\"{self.globaliter}_distance_error.png\"))\n","        plt.close()\n","        return np.mean(filtered)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nuYhU-MyRKrP"},"source":["# Trainer"]},{"cell_type":"code","metadata":{"id":"jhkdEc4SQZTE"},"source":["class Trainer:\n","\n","    def __init__(self, model, config, name):\n","\n","        # System configuration\n","        self.device = config.device\n","\n","        # Model Construction\n","        self.model = Network(config).float()\n","        self.model.load_state_dict(model)\n","        self.model.to(self.device)\n","        print(self.model)\n","\n","        # Peak detection and Evaluation\n","        self.eval_prediction = peak_detection.eval_prediction\n","        self.peak_comp = peak_detection.peak_comp\n","        self.plot_stats = peak_detection.plot_stats\n","        self.x = np.linspace(0, 150, num=150, endpoint=True)\n","        self.xnew = np.linspace(0, 150, num=512, endpoint=True)\n","        self.weight = np.ones(150) * 12\n","        self.output_dir = config.output_dir\n","        self.png_dir = config.png_dir\n","        # self.roc_dir = config.roc_dir\n","        self.globaliter = 0\n","\n","        # DataLoader\n","        self.test_loader = DataLoader(\n","            CoolDataset(r'/content/drive/MyDrive/Deep_learning/Deep Learning/Data/iteration3/Test'f\"/{name}\", config.seq_length,config.input_size,config.samples_per_event, convolution=True), batch_size=config.batch_size, drop_last=True,\n","            shuffle=False)\n","\n","    def test(self):\n","        self.model.eval()\n","        with torch.no_grad():\n","            for indx, data, target, filename in self.test_loader:\n","\n","                error_dist = []\n","                error_tpr =  []\n","                error_fpr =  []\n","                error_filename = []\n","                filename_end = []\n","\n","                data, target = data.to(self.device), target.to(self.device)\n","                predictions = self.model(data.float())\n","                pred = torch.sigmoid(predictions)\n","                # correct_indx_positive = pred[target > 0.5]  # Should have batch_size * 1's\n","                # correct_indx_negative = pred[target <= 0.5]  # Should have (batch_size*seq_length-batch_size) * 0'\n","                # TPR_temp = len(correct_indx_positive[correct_indx_positive > 0.5]) / len(correct_indx_positive)\n","                # TNR_temp = len(correct_indx_negative[correct_indx_negative <= 0.5]) / len(correct_indx_negative)\n","                # TPR_temp = TPR_temp * 100\n","                # TNR_temp = TNR_temp * 100\n","\n","                for i in range(0, pred.shape[0]):\n","\n","                    dist,tpr,fpr,filename_temp = (self.eval_prediction(self, pred[i], target[i], filename[i], i))\n","                    error_fpr.append(fpr)\n","                    error_tpr.append(tpr)\n","                    error_filename.append(filename_temp)\n","\n","                    for j in range(0, len(dist)):\n","                        error_dist.append(dist[j])\n","                        filename_end.append(filename[i])\n","\n","                # dist_file = np.column_stack((error_dist, filename_end))\n","                # fpr_tpr_file = np.column_stack((error_fpr, error_tpr, error_filename))\n","\n","                # mean_dist = self.plot_stats(self, error_dist)\n","\n","                # mean_name_dist = \"mean_dist\"\n","\n","                # mean_dist_file = np.column_stack((mean_dist, mean_name_dist))\n","                # dist_file = np.row_stack((dist_file, mean_dist_file))\n","\n","                # df_dist = pd.DataFrame(data=dist_file, )\n","                # df_fpr_tpr = pd.DataFrame(data=fpr_tpr_file, )\n","                # df_dist.columns = [\"Error in distance\", \"FileName\"]\n","                # df_fpr_tpr.columns = [\"FPR\",\"TPR\", \"FileName\"]\n","\n","                # df_dist.to_csv(os.path.join(self.output_dir, f\"{self.globaliter}_error_file.csv\"))\n","                # df_fpr_tpr.to_csv(os.path.join(self.output_dir, f\"{self.globaliter}_fpr_tpr_file.csv\"))\n","\n","                self.globaliter += 1\n","            # return TPR_temp, TNR_temp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5A-IrpodRNQf"},"source":["# Main"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VI9DAKmmRPwl","outputId":"16c3449c-2b6a-4738-f062-5cf6659d3827"},"source":["class Config:\n","\n","    def __init__(self, **kwargs):\n","        for key, value in kwargs.items():\n","            setattr(self, key, value)\n","\n","def main(loaded_model, model_config, name):\n","\n","    trainer = Trainer(loaded_model, model_config, name)\n","    trainer.test()\n","    # TPR_temp, TNR_temp = trainer.test()\n","    # return TPR_temp, TNR_temp\n","if __name__ == '__main__':\n","\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","        print(\"Running on the GPU\")\n","    else:\n","        device = torch.device(\"cpu\")\n","        print(\"Running on the CPU\")\n","\n","    # Calling Models\n","    names    = [\"ForeFoot\",\"MidFoot\",\"Heel\"]\n","\n","    model_dir = r'/content/drive/MyDrive/Deep_learning/Deep Learning/Results/LSTM/IC/2markers/TOEHEE/v1/models'\n","    model_files = tuple(Path(f\"{model_dir}/CheckPoint_DS/\").glob(\"**/*pt\"))\n","\n","    for name in names:\n","      # ROC_Final = []\n","      # TPR_Final = []\n","      # TNR_Final = []\n","      # roc_dir       = f\"{model_dir}/roc_TS/{name}/\"\n","      # if not os.path.exists(roc_dir):\n","          # os.makedirs(roc_dir)\n","\n","      for i in range(len(model_files)):\n","          model_name = os.path.basename((model_files[i]))\n","          loaded_model = torch.load(f\"{model_dir}/CheckPoint_DS/\" + model_name,map_location=torch.device(device))\n","\n","          output_dir    = f\"{model_dir}/evaluation/{name}/Results_final{i}/csv\"\n","          png_dir       = f\"{model_dir}/evaluation/{name}/Results_final{i}/png\"\n","          print(model_name)\n","\n","          # Directory Settings\n","          if not os.path.exists(output_dir):\n","              os.makedirs(output_dir)\n","              os.makedirs(png_dir)\n","\n","          model_config = Config(\n","\n","              # General Configuration\n","              device=device,\n","              batch_size=32,\n","              seq_length=150,\n","              samples_per_event=1,\n","              output_size=1,\n","\n","              # LSTM\n","              input_size=12,\n","              hidden_size=256,\n","              num_layers=5,\n","              lr=0.001,\n","              drop_out=0.3,\n","\n","              output_dir=output_dir,\n","              png_dir=png_dir,\n","              # roc_dir=roc_dir\n","          )\n","\n","          # ROC =\n","          main(loaded_model, model_config, name)\n","          # TPR, TNR = main(loaded_model, model_config, name)\n","          # TPR_Final.append(TPR)\n","          # TNR_Final.append(TNR)\n","      # ROC = pd.DataFrame({f\"TPR_{name}\": TPR_Final, f\"TNR_{name}\": TNR_Final})\n","      # ROC.to_csv(os.path.join(roc_dir, f\"{name}_ROC.csv\"), index=False)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running on the CPU\n","IC-256-HS-5-NL-0.001-LR-0.3-DO-1-FScheckpoint.pt\n","Network(\n","  (lstm): LSTM(12, 256, num_layers=5, batch_first=True, dropout=0.3, bidirectional=True)\n","  (linear): Linear(in_features=512, out_features=1, bias=True)\n",")\n","IC-256-HS-5-NL-0.001-LR-0.3-DO-2-FScheckpoint.pt\n","Network(\n","  (lstm): LSTM(12, 256, num_layers=5, batch_first=True, dropout=0.3, bidirectional=True)\n","  (linear): Linear(in_features=512, out_features=1, bias=True)\n",")\n","RT_EVT059_09.csv - No Events\n","IC-256-HS-5-NL-0.001-LR-0.3-DO-3-FScheckpoint.pt\n","Network(\n","  (lstm): LSTM(12, 256, num_layers=5, batch_first=True, dropout=0.3, bidirectional=True)\n","  (linear): Linear(in_features=512, out_features=1, bias=True)\n",")\n","RT_EVT061_13.csv - No Events\n","RT_EVT060_10.csv - No Events\n","RT_EVT128_08.csv - No Events\n","RT_EVT128_11.csv - No Events\n","RT_EVT207_10.csv - No Events\n","RT_EVT208_08.csv - No Events\n","RT_EVT261_14.csv - No Events\n","LT_EVT061_06.csv - No Events\n","LT_EVT060_12.csv - No Events\n","LT_EVT060_12.csv - No Events\n","LT_EVT208_09.csv - No Events\n","LT_EVT240_06.csv - No Events\n","IC-256-HS-5-NL-0.001-LR-0.3-DO-4-FScheckpoint.pt\n","Network(\n","  (lstm): LSTM(12, 256, num_layers=5, batch_first=True, dropout=0.3, bidirectional=True)\n","  (linear): Linear(in_features=512, out_features=1, bias=True)\n",")\n","IC-256-HS-5-NL-0.001-LR-0.3-DO-5-FScheckpoint.pt\n","Network(\n","  (lstm): LSTM(12, 256, num_layers=5, batch_first=True, dropout=0.3, bidirectional=True)\n","  (linear): Linear(in_features=512, out_features=1, bias=True)\n",")\n"]}]},{"cell_type":"code","metadata":{"id":"lmGEGgMVtDd_"},"source":[""],"execution_count":null,"outputs":[]}]}